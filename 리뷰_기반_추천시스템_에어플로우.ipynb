{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "리뷰 기반 추천시스템 - 에어플로우.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNapYWjBohLKsLFQ8Hqe9Kc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**수정해야 할 것** <br/>\n",
        "해당 코드 옆에 `# 수정` 이라고 주석 달아놓겠습니다!\n",
        "- `# 데이터 로드` 셀의 `critic_data`, `user_data`, `steam_review` read_csv\n",
        "- `Doc2Vec` 셀의 model.save : 경로 수정"
      ],
      "metadata": {
        "id": "kCH7dW_LkOlg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEdjucLGjRvm"
      },
      "outputs": [],
      "source": [
        "# 패키지 설치\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.models import load_model\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "import re\n",
        "import nltk\n",
        "import joblib\n",
        "import pickle\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수 모음\n",
        "\n",
        "# 특수문자 제거\n",
        "def remain_engnum(sentence):\n",
        "  sentence = re.sub('[^a-zA-Z0-9]+', ' ', sentence)\n",
        "  return sentence\n",
        "\n",
        "# 희귀단어 제거\n",
        "def remain_word(review):\n",
        "  result = []\n",
        "  for word in review:\n",
        "    if word in total_tokenizer.word_index.keys():\n",
        "      result.append(word)\n",
        "  return result\n",
        "\n",
        "# 불용어 제거\n",
        "def remove_stopwords(text): \n",
        "    output= [i for i in text if i not in stop_words] \n",
        "    return output\n",
        "\n",
        "# 추천 게임 출력\n",
        "def gentleman_ver1(game1, game2, game3):\n",
        "  d1 = pd.DataFrame(model.docvecs.most_similar([game1], topn=10), columns=['game', 'similarity1']) # game1 추천목록 & 유사도\n",
        "  d2 = pd.DataFrame(model.docvecs.most_similar([game2], topn=10), columns=['game', 'similarity2']) # game2 추천목록 & 유사도\n",
        "  d3 = pd.DataFrame(model.docvecs.most_similar([game3], topn=10), columns=['game', 'similarity3']) # game3 추천목록 & 유사도\n",
        "  df = pd.merge(d1, d2, on='game', how='outer') # d1, d2 merge\n",
        "  df = pd.merge(df, d3, on='game', how='outer') # d2, d3 merge\n",
        "\n",
        "  for game in df['game']: # ['game'] 행 하나씩 돌면서 \n",
        "    if game in [game1, game2, game3]: # game이 game1, game2,m game3에 해당하는 경우\n",
        "      drop_index = df[df['game']==game].index\n",
        "      df.drop(drop_index, axis='index', inplace=True) # 해당 행 drop\n",
        "\n",
        "  # 추천 리스트 간 중복되는 게임이 없는 경우 각 추천리스트에서 2개씩 뽑음\n",
        "  if len(df) == 30: \n",
        "    return [x[0] for x in model.docvecs.most_similar([game1], topn=2)] + [x[0] for x in model.docvecs.most_similar([game2], topn=2)] + [x[0] for x in model.docvecs.most_similar([game3], topn=2)]\n",
        "  # 유사도 평균 내서 상위 6개 추출\n",
        "  else : \n",
        "    df.fillna(0, inplace=True)\n",
        "    df['mean'] = df.mean(axis=1, numeric_only=True)\n",
        "    df.sort_values('mean', ascending=False, inplace=True)\n",
        "    return list(df['game'])[:6]"
      ],
      "metadata": {
        "id": "_UIazZ4yk80i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 메타 전문가 리뷰\n",
        "\n",
        "critic_data = pd.read_csv('./data/meta_final/metacrawl_meta_final.csv') # 수정\n",
        "\n",
        "# game / review 열로 가져오기, 조건 설정\n",
        "d1 = critic_data[(critic_data['platform']=='pc') & (critic_data['criticscore']>=75)][['title', 'criticcontent']]\n",
        "\n",
        "# 이름 각각 game, score으로 변경\n",
        "d1.rename(columns = {'title':'game', 'criticcontent':'review'}, inplace = True)\n",
        "# 리뷰 NaN값인 행 제거\n",
        "d1.dropna(subset=['review'], inplace=True) \n",
        "\n",
        "# 게임 이름에 전처리 : 특수문자 없애기, 띄어쓰기 없애기, 소문자 처리하기\n",
        "d1['game'] = d1['game'].apply(lambda x:remain_engnum(x))\n",
        "d1['game'] = d1['game'].str.replace(\" \", \"\")\n",
        "d1['game'] = d1['game'].str.lower()\n",
        "\n",
        "\n",
        "\n",
        "# 메타 유저 리뷰\n",
        "\n",
        "user_data = pd.read_csv(\"./data/meta_final/metacrawl_user_final.csv\") # 수정\n",
        "\n",
        "# game / review 열로 가져오기, 조건 설정\n",
        "d2 = user_data[(user_data['platform']=='pc') & (user_data['userscore']>7.5)][['title', 'usercontent']]\n",
        "\n",
        "# 이름 각각 user, game, score으로 변경\n",
        "d2.rename(columns = {'title':'game', 'usercontent':'review'}, inplace = True)\n",
        "# 리뷰 NaN값인 행 제거\n",
        "d2.dropna(subset=['review'], inplace=True) \n",
        "\n",
        "# 게임 이름에 전처리 : 특수문자 없애기, 띄어쓰기 없애기, 소문자 처리하기\n",
        "d2['game'] = d2['game'].apply(lambda x:remain_engnum(x))\n",
        "d2['game'] = d2['game'].str.replace(\" \", \"\")\n",
        "d2['game'] = d2['game'].str.lower()\n",
        "\n",
        "\n",
        "\n",
        "# 스팀 리뷰\n",
        "\n",
        "steam_review = pd.read_csv(\"./data/steam_data/real_final.csv\") # 수정\n",
        "\n",
        "steam_review.dropna(subset=['review'], inplace=True) # 리뷰 NaN값인 행 제거\n",
        "steam_review.dropna(subset=['weighted_vote_score'], inplace=True) # weighted_vote_score NaN값인 행 제거\n",
        "steam_review.dropna(subset=['voted_up'], inplace=True) # voted_up NaN값인 행 제거\n",
        "steam_review.dropna(subset=['title'], inplace=True) # title NaN값인 행 제거\n",
        "\n",
        "d3 = steam_review[(steam_review['voted_up']==True) & (steam_review['weighted_vote_score']>=0.75)][['title', 'review']]\n",
        "\n",
        "# 이름 각각 game, score으로 변경\n",
        "d3.rename(columns = {'title':'game'}, inplace = True)\n",
        "\n",
        "# 게임 이름에 전처리 : 특수문자 없애기, 띄어쓰기 없애기, 소문자 처리하기\n",
        "d3['game'] = d3['game'].apply(lambda x:remain_engnum(x))\n",
        "d3['game'] = d3['game'].str.replace(\" \", \"\")\n",
        "d3['game'] = d3['game'].str.lower()\n",
        "\n",
        "\n",
        "\n",
        "# total_data  생성 : d1, d2, d3 합치기\n",
        "total_data = pd.concat([d1, d2, d3])\n",
        "# nan값 있는 행 제거\n",
        "total_data.dropna(inplace=True)\n",
        "# 게임 명 공백인 행 제거\n",
        "drop_index = total_data[total_data['game']==''].index\n",
        "total_data.drop(drop_index, axis='index', inplace=True)"
      ],
      "metadata": {
        "id": "S_h1-9chkLCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 희귀단어 기준 계산\n",
        "# tokenizer로 total_data['review'] 단어사전 만들고 희귀단어 비율 보기\n",
        "\n",
        "tok = Tokenizer() # 토크나이저 할당\n",
        "tok.fit_on_texts(total_data['review']) # 리뷰 데이터로 토크나이저 학습\n",
        "\n",
        "# 등장 빈도수가 3회 이하 단어를 희귀단어로 취급\n",
        "threshold = 3\n",
        "total_cnt = len(tok.word_index) # 토크나이저가 생성한 단어사전의 단어 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "\n",
        "for key, value in tok.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "    if(value <= threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "\n",
        "# 희귀단어 기준\n",
        "vocab_size = total_cnt- rare_cnt + 1 "
      ],
      "metadata": {
        "id": "-Yq0mXaFlaSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리뷰 전처리\n",
        "\n",
        "# 영어, 숫자만 빼고 제거\n",
        "total_data['review'] = total_data['review'].apply(lambda x:remain_engnum(x))\n",
        "\n",
        "# 소문자화\n",
        "total_data['review']= total_data['review'].apply(lambda x: x.lower())\n",
        "\n",
        "# 게임 별로 리뷰 합치기\n",
        "game_review = pd.DataFrame(total_data['game'].unique(), columns=['game'])\n",
        "\n",
        "for game in total_data['game'].unique():\n",
        "  game_review.loc[game_review['game']==game, 'review'] = \" \".join(total_data[total_data['game']==game]['review'])\n",
        "\n",
        "# 희귀단어 제거를 위한 토크나이저 생성\n",
        "total_tokenizer = Tokenizer(num_words = vocab_size) # 희귀단어 제거한 단어사전 생성\n",
        "total_tokenizer.fit_on_texts(game_review['review']) # 학습\n",
        "\n",
        "# 단어사전에서 희귀단어 제거\n",
        "words_frequency = [word for word, index in total_tokenizer.word_index.items() if index >= vocab_size + 1]\n",
        "for word in words_frequency:\n",
        "    del total_tokenizer.word_index[word] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "    del total_tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제\n",
        "\n",
        "# 토큰화\n",
        "game_review['review'] = game_review['review'].apply(lambda x:text_to_word_sequence(x))\n",
        "\n",
        "# 희귀단어 제거 \n",
        "game_review['review'] = game_review['review'].apply(lambda x:remain_word(x))\n",
        "\n",
        "# 불용어 제거\n",
        "stop_words = set(stopwords.words('english'))\n",
        "game_review['review'] = game_review['review'].apply(lambda x:remove_stopwords(x))"
      ],
      "metadata": {
        "id": "WQ3lv8vDpGOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Doc2Vec\n",
        "\n",
        "\n",
        "# Doc2Vec에 들어가는 최종 데이터 TaggedDocument 생성\n",
        "\n",
        "doc_df = game_review[['game','review']].values.tolist()\n",
        "tagged_data = [TaggedDocument(words=_d, tags=[gid]) for gid, _d in doc_df]\n",
        "\n",
        "\n",
        "# Doc2Vec 학습\n",
        "max_epochs = 10\n",
        "\n",
        "model = Doc2Vec(\n",
        "    window=5, # 모델 학습 시 앞 뒤로 보는 단어 개수\n",
        "    size=100, # 벡터차원 크기\n",
        "    alpha=0.025, # learning rate\n",
        "    min_alpha=0.025,# min learning rate. 학습 진행될수록 alpha값이 min_alpha로 근사\n",
        "    min_count=3, # 학습에 사용할 최소 단어 빈도 수\n",
        "    dm =1, # 학습 방법 : 1-pvdm, 0-pvdbow\n",
        "    negative = 5, # complexity reudction. negative sampling 시 샘플링 할 단어 개수 지정\n",
        "    seed = 9999)\n",
        "\n",
        "model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "model.save('./model/finaldatadoc2vec.model') # 수정"
      ],
      "metadata": {
        "id": "qtGJKfkwqkia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델 불러오기\n",
        "\n",
        "model = Doc2Vec.load('./model/finaldatadoc2vec.model') # 수정"
      ],
      "metadata": {
        "id": "QnFdgsUFulAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bOoPogFau58m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}