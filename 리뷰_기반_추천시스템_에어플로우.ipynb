{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "리뷰 기반 추천시스템 - 에어플로우.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHWDnhTXz4DeLoyX3O/df1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helloitsmimi/helloitsmimi/blob/master/%EB%A6%AC%EB%B7%B0_%EA%B8%B0%EB%B0%98_%EC%B6%94%EC%B2%9C%EC%8B%9C%EC%8A%A4%ED%85%9C_%EC%97%90%EC%96%B4%ED%94%8C%EB%A1%9C%EC%9A%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch0uDOG6cel7",
        "outputId": "8574f434-bde9-4e45-c76c-9057cecbfd2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/멀티캠퍼스 파이썬/최종 프로젝트\n"
          ]
        }
      ],
      "source": [
        "#드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#현재 작업 위치 이동\n",
        "#띄어쓰기에 \\붙일 것\n",
        "%cd /content/drive/MyDrive/멀티캠퍼스 파이썬/최종 프로젝트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEdjucLGjRvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c639758-4f1b-4235-aa70-20201091db9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# 패키지 설치\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.models import load_model\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "import re\n",
        "import nltk\n",
        "import joblib\n",
        "import pickle\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_data():\n",
        "\n",
        "  '''\n",
        "  유저/게임/리뷰 데이터를 이용해\n",
        "  최종적으로 Doc2Vec에 사용되는 데이터 형태인 TaggedDocument 만들기\n",
        "  '''\n",
        "\n",
        "  # 메타 전문가 리뷰 불러오기\n",
        "  critic_data = pd.read_csv('./data/meta_final/meta_meta_mart.csv')\n",
        "  # game / review 열로 가져오기, 조건 설정\n",
        "  d1 = critic_data[(critic_data['platform']=='pc') & (critic_data['criticscore']>=75)][['appid', 'criticcontent']]\n",
        "  # 이름 각각 game, score으로 변경\n",
        "  d1.rename(columns = {'appid':'game', 'criticcontent':'review'}, inplace = True)\n",
        "  # 리뷰 NaN값인 행 제거\n",
        "  d1.dropna(subset=['review'], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "  # 메타 유저 리뷰 불러오기\n",
        "  user_data = pd.read_csv(\"./data/meta_final/meta_user_mart.csv\")\n",
        "  # game / review 열로 가져오기, 조건 설정\n",
        "  d2 = user_data[(user_data['platform']=='pc') & (user_data['userscore']>7.5)][['appid', 'usercontent']]\n",
        "  # 이름 각각 user, game, score으로 변경\n",
        "  d2.rename(columns = {'appid':'game', 'usercontent':'review'}, inplace = True)\n",
        "  # 리뷰 NaN값인 행 제거\n",
        "  d2.dropna(subset=['review'], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "  # 스팀 리뷰 데이터 불러오기\n",
        "  steam_review = pd.read_csv(\"./data/steam_data/for_mimi.csv\")\n",
        "  steam_review.dropna(subset=['review'], inplace=True) # 리뷰 NaN값인 행 제거\n",
        "  steam_review.dropna(subset=['weighted_vote_score'], inplace=True) # weighted_vote_score NaN값인 행 제거\n",
        "  steam_review.dropna(subset=['voted_up'], inplace=True) # voted_up NaN값인 행 제거\n",
        "  steam_review.dropna(subset=['appid'], inplace=True) # appid NaN값인 행 제거\n",
        "  d3 = steam_review[(steam_review['voted_up']==True) & (steam_review['weighted_vote_score']>=0.75)][['appid', 'review']]\n",
        "  # 이름 각각 game, score으로 변경\n",
        "  d3.rename(columns = {'appid':'game'}, inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "  # d1, d2, d3 합치기\n",
        "  total_data = pd.concat([d1, d2, d3])\n",
        "  # nan값 있는 행 제거\n",
        "  total_data.dropna(inplace=True)\n",
        "  # 게임 명 공백인 행 제거\n",
        "  drop_index = total_data[total_data['game']==''].index\n",
        "  total_data.drop(drop_index, axis='index', inplace=True)\n",
        "  # game열 타입 float에서 int로\n",
        "  total_data = total_data.astype({'game':'int'})\n",
        "\n",
        "\n",
        "\n",
        "  # 자연어 처리를 위한 토크나이저 생성\n",
        "  # 토크나이저의 단어 사전 : 희귀단어 제거하고 생성\n",
        "  # tokenizer로 total_data['review'] 단어사전 만들고 희귀단어 비율 보기\n",
        "\n",
        "  tok = Tokenizer() # 토크나이저 할당\n",
        "  tok.fit_on_texts(total_data['review']) # 리뷰 데이터로 토크나이저 학습\n",
        "\n",
        "  # 등장 빈도수가 3회 이하 단어를 희귀단어로 취급\n",
        "  threshold = 3\n",
        "  total_cnt = len(tok.word_index) # 토크나이저가 생성한 단어사전의 단어 수\n",
        "  rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "  total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "\n",
        "  for key, value in tok.word_counts.items():\n",
        "      total_freq = total_freq + value\n",
        "      if(value <= threshold):\n",
        "          rare_cnt = rare_cnt + 1\n",
        "\n",
        "  # vocab_size : 희귀단어 count 기준\n",
        "  vocab_size = total_cnt- rare_cnt + 1 \n",
        "\n",
        "\n",
        "\n",
        "  # 특수문자 제거\n",
        "  def remain_engnum(sentence):\n",
        "    sentence = re.sub('[^a-zA-Z0-9]+', ' ', sentence)\n",
        "    return sentence\n",
        "\n",
        "  total_data['review'] = total_data['review'].apply(lambda x:remain_engnum(x))\n",
        "\n",
        "  # 소문자화\n",
        "  total_data['review']= total_data['review'].apply(lambda x: x.lower())\n",
        "\n",
        "  # 게임 별로 리뷰 합치기\n",
        "  game_review = pd.DataFrame(total_data['game'].unique(), columns=['game'])\n",
        "\n",
        "  for game in total_data['game'].unique():\n",
        "    game_review.loc[game_review['game']==game, 'review'] = \" \".join(total_data[total_data['game']==game]['review'])\n",
        "\n",
        "  # 희귀단어 제거를 위한 토크나이저 생성\n",
        "  total_tokenizer = Tokenizer(num_words = vocab_size) # 희귀단어 제거한 단어사전 생성\n",
        "  total_tokenizer.fit_on_texts(game_review['review']) # 학습\n",
        "\n",
        "  # 단어사전에서 희귀단어 제거\n",
        "  words_frequency = [word for word, index in total_tokenizer.word_index.items() if index >= vocab_size + 1]\n",
        "  for word in words_frequency:\n",
        "      del total_tokenizer.word_index[word] # 해당 단어에 대한 인덱스 정보를 삭제\n",
        "      del total_tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제\n",
        "\n",
        "  # 토큰화\n",
        "  game_review['review'] = game_review['review'].apply(lambda x:text_to_word_sequence(x))\n",
        "\n",
        "  # 희귀단어 제거\n",
        "  def remain_word(review):\n",
        "    result = []\n",
        "    for word in review:\n",
        "      if word in total_tokenizer.word_index.keys():\n",
        "        result.append(word)\n",
        "    return result\n",
        "     \n",
        "  game_review['review'] = game_review['review'].apply(lambda x:remain_word(x))\n",
        "\n",
        "  # 불용어 제거\n",
        "  def remove_stopwords(text): \n",
        "    output= [i for i in text if i not in stop_words] \n",
        "    return output\n",
        "    \n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  game_review['review'] = game_review['review'].apply(lambda x:remove_stopwords(x))\n",
        "\n",
        "  doc_df = game_review[['game','review']].values.tolist()\n",
        "  tagged_data = [TaggedDocument(words=_d, tags=[gid]) for gid, _d in doc_df]\n",
        "\n",
        "  return tagged_data"
      ],
      "metadata": {
        "id": "ZZ5d-Ej1EViD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Doc2Vec\n",
        "\n",
        "def make_model(tagged_data):\n",
        "\n",
        "  '''\n",
        "  make_data()로 생성한 tagged_data를 사용해서\n",
        "  Doc2Vec으로 리뷰 간 유사도 계산\n",
        "\n",
        "  make_data()를 먼저 변수에 할당하고\n",
        "  make_model()에 매개변수로 입력\n",
        "  '''\n",
        "\n",
        "  # Doc2Vec 학습\n",
        "  max_epochs = 10 # epoch 10번\n",
        "\n",
        "  model = Doc2Vec(\n",
        "    window=5, # 모델 학습 시 앞 뒤로 보는 단어 개수\n",
        "    vector_size=100, # 벡터차원 크기\n",
        "    alpha=0.025, # learning rate\n",
        "    min_alpha=0.025,# min learning rate. 학습 진행될수록 alpha값이 min_alpha로 근사\n",
        "    min_count=3, # 학습에 사용할 최소 단어 빈도 수\n",
        "    dm =1, # 학습 방법 : 1-pvdm, 0-pvdbow\n",
        "    negative = 5, # complexity reudction. negative sampling 시 샘플링 할 단어 개수 지정\n",
        "    seed = 9999)\n",
        "\n",
        "  model.build_vocab(tagged_data) # 단어사전 생성\n",
        "\n",
        "  for epoch in range(max_epochs): # tagged_data로 10번 학습\n",
        "      print('iteration {0}'.format(epoch))\n",
        "      model.train(tagged_data, # 학습에 사용할 데이터\n",
        "                  total_examples=model.corpus_count,\n",
        "                  epochs=model.epochs)\n",
        "     # decrease the learning rate\n",
        "      model.alpha -= 0.002\n",
        "     # fix the learning rate, no decay\n",
        "      model.min_alpha = model.alpha\n",
        "\n",
        "  model.save('./model/finaldatadoc2vec.model') # 수정"
      ],
      "metadata": {
        "id": "54xr9PA1YOTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Doc2Vec.load('./model/finaldatadoc2vec.model') # make_model()에서 저장했던 모델 불러오기"
      ],
      "metadata": {
        "id": "PeMeyoFl6OYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gentleman_ver1(game1, game2, game3):\n",
        "\n",
        "  '''\n",
        "  추천 게임 출력 함수\n",
        "  '''\n",
        "\n",
        "\n",
        "  d1 = pd.DataFrame(model.docvecs.most_similar([game1], topn=10), columns=['game', 'similarity1']) # game1 추천목록 & 유사도\n",
        "  d2 = pd.DataFrame(model.docvecs.most_similar([game2], topn=10), columns=['game', 'similarity2']) # game2 추천목록 & 유사도\n",
        "  d3 = pd.DataFrame(model.docvecs.most_similar([game3], topn=10), columns=['game', 'similarity3']) # game3 추천목록 & 유사도\n",
        "  df = pd.merge(d1, d2, on='game', how='outer') # d1, d2 merge\n",
        "  df = pd.merge(df, d3, on='game', how='outer') # d2, d3 merge\n",
        "\n",
        "  for game in df['game']: # ['game'] 행 하나씩 돌면서 \n",
        "    if game in [game1, game2, game3]: # game이 game1, game2,m game3에 해당하는 경우\n",
        "      drop_index = df[df['game']==game].index\n",
        "      df.drop(drop_index, axis='index', inplace=True) # 해당 행 drop\n",
        "\n",
        "  # 추천 리스트 간 중복되는 게임이 없는 경우 : 각 추천리스트에서 2개씩 추출\n",
        "  if len(df) == 30: \n",
        "    return [x[0] for x in model.docvecs.most_similar([game1], topn=2)] + [x[0] for x in model.docvecs.most_similar([game2], topn=2)] + [x[0] for x in model.docvecs.most_similar([game3], topn=2)]\n",
        "  # 추천 리스트 간 중복되는 게임 있는 경우 : 유사도 평균 내서 상위 6개 추출\n",
        "  else : \n",
        "    df.fillna(0, inplace=True)\n",
        "    df['mean'] = df.mean(axis=1, numeric_only=True)\n",
        "    df.sort_values('mean', ascending=False, inplace=True)\n",
        "    return list(df['game'])[:6]"
      ],
      "metadata": {
        "id": "bOoPogFau58m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gentleman_ver1(1445990, 728880, 546560)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GOSSrbPj8fT",
        "outputId": "59fe888d-20ed-481c-866a-61b47d7ae3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1915840, 387020, 448510, 1097770, 70, 220]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}