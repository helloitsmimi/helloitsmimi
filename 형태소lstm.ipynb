{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "형태소lstm 합친거.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8kWTUSlDsg6",
        "outputId": "531cfa6e-5edb-421a-f96e-62fec968851a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 65.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymysql\n",
            "  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 225 kB/s \n",
            "\u001b[?25hInstalling collected packages: pymysql\n",
            "Successfully installed pymysql-1.0.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 10.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=f37551fded54f004babdc3f903a9f84e0f1e46113b70e7c42fcacde8f06a89df\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy\n",
        "!pip install pymysql\n",
        "!pip install emoji\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import konlpy\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pymysql\n",
        "import emoji\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM , Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "metadata": {
        "id": "M8Dp5CyEFEMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_emojis(text):\n",
        "    for c in text:\n",
        "        if c in emoji.UNICODE_EMOJI['en']:\n",
        "          text = re.sub(c, \"\", text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 {} 이하인 샘플의 비율: {}'.format(max_len, (cnt / len(nested_list))*100))\n",
        "\n",
        "def mean_word_predict_morpheme(new_sentence):\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "  score = float(loaded_model_morpheme.predict(pad_new)) # 예측\n",
        "  if(score > 0.5):\n",
        "    print(\"{:.2f}% 확률로 욕설입니다.\\n\".format(score * 100))\n",
        "  else:\n",
        "    print(\"{:.2f}% 확률로 욕설이 아닙니다.\\n\".format((1 - score) * 100))"
      ],
      "metadata": {
        "id": "lkdrJxZvFgeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "okt.morphs('이 바보야 진짜 아니야', stem=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05BHEYPlP-l2",
        "outputId": "8427151d-17e8-49f5-f3ec-82c1f71385d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['이', '바보', '야', '진짜', '아니다']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conn = pymysql.connect(\n",
        "    user='study',\n",
        "    passwd='hellobee1234',\n",
        "    host='121.134.135.223',\n",
        "    port=3306,\n",
        "    db='data'\n",
        ")\n",
        "\n",
        "cursor = conn.cursor(pymysql.cursors.DictCursor)\n",
        "sql = \"SELECT * FROM data.dataset;\"\n",
        "cursor.execute(sql)\n",
        "result = cursor.fetchall()\n",
        "data = pd.DataFrame(result)\n",
        "data.dropna(inplace=True)\n",
        "data['records'] = data['records'].map(extract_emojis)\n",
        "data['records'] = data['records'].str.replace(r'\\[\\d+\\]', \"\")\n",
        "data['records'] = data['records'].str.replace(\"\\xa0\", \"\")\n",
        "data.dropna(inplace=True)\n",
        "data.drop_duplicates(subset=['records'], inplace=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['records'], data['label'], test_size = 0.3, random_state = 123)\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "train_record = []\n",
        "for sentence in tqdm(X_train):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    train_record.append(tokenized_sentence)\n",
        "\n",
        "test_record = []\n",
        "for sentence in tqdm(X_test):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    test_record.append(tokenized_sentence)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_record)\n",
        " \n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "\n",
        "tokenizer = Tokenizer(total_cnt) # 희귀단어 집합 제거 안 함\n",
        "tokenizer.fit_on_texts(train_record)\n",
        "\n",
        "with open('wordlstm.pkl', 'wb') as f:\n",
        "\tpickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#텍스트 시퀀스를 정수 시퀀스로 변환\n",
        "morpheme_X_train = tokenizer.texts_to_sequences(train_record) \n",
        "morpheme_X_test = tokenizer.texts_to_sequences(test_record)\n",
        "\n",
        "max_len = 20\n",
        "below_threshold_len(max_len, morpheme_X_train)\n",
        "\n",
        "morpheme_X_train = pad_sequences(morpheme_X_train, maxlen = max_len)\n",
        "morpheme_X_test = pad_sequences(morpheme_X_test, maxlen = max_len)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_cnt, 100)) \n",
        "model.add(LSTM(100, dropout=0.6))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(patience=4,restore_best_weights=True)\n",
        "mc_m = ModelCheckpoint('./model/best_model_mean_words_morpheme.h5', save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(morpheme_X_train, y_train, epochs=100, callbacks=[es, mc_m], batch_size=16, validation_split=0.2)\n",
        "\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(morpheme_X_test, y_test)[1]))\n",
        "\n",
        "loaded_model_morpheme = load_model('./model/best_model_mean_words_morpheme.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0JVmEAjFSZz",
        "outputId": "6084dcb7-7cf0-48e7-9eff-a67b09ca4573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  app.launch_new_instance()\n",
            "100%|██████████| 39750/39750 [00:52<00:00, 755.96it/s] \n",
            "100%|██████████| 17037/17037 [00:15<00:00, 1088.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 중 길이가 20 이하인 샘플의 비율: 99.78867924528302\n",
            "Epoch 1/100\n",
            "1988/1988 [==============================] - 24s 8ms/step - loss: 0.2746 - accuracy: 0.8851 - val_loss: 0.1988 - val_accuracy: 0.9255\n",
            "Epoch 2/100\n",
            "1988/1988 [==============================] - 16s 8ms/step - loss: 0.1366 - accuracy: 0.9497 - val_loss: 0.2008 - val_accuracy: 0.9220\n",
            "Epoch 3/100\n",
            "1988/1988 [==============================] - 15s 7ms/step - loss: 0.0956 - accuracy: 0.9652 - val_loss: 0.2055 - val_accuracy: 0.9281\n",
            "Epoch 4/100\n",
            "1988/1988 [==============================] - 14s 7ms/step - loss: 0.0725 - accuracy: 0.9746 - val_loss: 0.2387 - val_accuracy: 0.9273\n",
            "Epoch 5/100\n",
            "1988/1988 [==============================] - 14s 7ms/step - loss: 0.0578 - accuracy: 0.9805 - val_loss: 0.2476 - val_accuracy: 0.9233\n",
            "533/533 [==============================] - 2s 3ms/step - loss: 0.2023 - accuracy: 0.9255\n",
            "\n",
            " Accuracy: 0.9255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_cnt, 100)) \n",
        "model.add(LSTM(100, dropout=0.6))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "es = EarlyStopping(patience=4,restore_best_weights=True)\n",
        "mc_m = ModelCheckpoint('./model/best_model_mean_words_morpheme.h5', save_best_only=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(morpheme_X_train, y_train, epochs=100, callbacks=[es, mc_m], batch_size=16, validation_split=0.2)\n",
        "\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(morpheme_X_test, y_test)[1]))\n",
        "\n",
        "loaded_model_morpheme = load_model('./model/best_model_mean_words_morpheme.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Q94G4NGlZw",
        "outputId": "42261039-f3ec-44f8-dfae-fccbad3ab75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1988/1988 [==============================] - 19s 8ms/step - loss: 0.2733 - accuracy: 0.8858 - val_loss: 0.1935 - val_accuracy: 0.9282\n",
            "Epoch 2/100\n",
            "1988/1988 [==============================] - 16s 8ms/step - loss: 0.1385 - accuracy: 0.9494 - val_loss: 0.1839 - val_accuracy: 0.9316\n",
            "Epoch 3/100\n",
            "1988/1988 [==============================] - 14s 7ms/step - loss: 0.0965 - accuracy: 0.9653 - val_loss: 0.2018 - val_accuracy: 0.9272\n",
            "Epoch 4/100\n",
            "1988/1988 [==============================] - 14s 7ms/step - loss: 0.0728 - accuracy: 0.9742 - val_loss: 0.2112 - val_accuracy: 0.9230\n",
            "Epoch 5/100\n",
            "1988/1988 [==============================] - 14s 7ms/step - loss: 0.0581 - accuracy: 0.9800 - val_loss: 0.2369 - val_accuracy: 0.9209\n",
            "Epoch 6/100\n",
            "1988/1988 [==============================] - 14s 7ms/step - loss: 0.0488 - accuracy: 0.9830 - val_loss: 0.2549 - val_accuracy: 0.9234\n",
            "533/533 [==============================] - 2s 3ms/step - loss: 0.1917 - accuracy: 0.9322\n",
            "\n",
            " Accuracy: 0.9322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_word_predict_morpheme('시발')\n",
        "mean_word_predict_morpheme('시2발')\n",
        "mean_word_predict_morpheme('씨2발')\n",
        "mean_word_predict_morpheme('시3발')\n",
        "mean_word_predict_morpheme('씨3발')\n",
        "mean_word_predict_morpheme('시4발')\n",
        "mean_word_predict_morpheme('씨4발')\n",
        "mean_word_predict_morpheme('시5발')\n",
        "mean_word_predict_morpheme('씨5발')\n",
        "mean_word_predict_morpheme('시6발')\n",
        "mean_word_predict_morpheme('씨6발')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzn8ZOUzGruD",
        "outputId": "3c6ff36c-664b-43de-a27e-7bdbaf026f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99.91% 확률로 욕설입니다.\n",
            "\n",
            "50.84% 확률로 욕설입니다.\n",
            "\n",
            "62.81% 확률로 욕설입니다.\n",
            "\n",
            "64.86% 확률로 욕설이 아닙니다.\n",
            "\n",
            "52.21% 확률로 욕설이 아닙니다.\n",
            "\n",
            "65.54% 확률로 욕설이 아닙니다.\n",
            "\n",
            "53.24% 확률로 욕설이 아닙니다.\n",
            "\n",
            "62.47% 확률로 욕설이 아닙니다.\n",
            "\n",
            "50.22% 확률로 욕설입니다.\n",
            "\n",
            "60.99% 확률로 욕설이 아닙니다.\n",
            "\n",
            "51.46% 확률로 욕설입니다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_word_predict_morpheme('ㅅㅐㄲㅣ')\n",
        "mean_word_predict_morpheme('ㅅㅐ끼')\n",
        "mean_word_predict_morpheme('ㄱㅐㅅㅐㄲㅣ')"
      ],
      "metadata": {
        "id": "uB_Tq3H3hN51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81487a21-07ef-4b2b-bc47-b88bb88d33d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86.95% 확률로 욕설이 아닙니다.\n",
            "\n",
            "82.85% 확률로 욕설이 아닙니다.\n",
            "\n",
            "86.95% 확률로 욕설이 아닙니다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_word_predict_morpheme('시발')\n",
        "mean_word_predict_morpheme('시빨')\n",
        "mean_word_predict_morpheme('시팔')\n",
        "mean_word_predict_morpheme('시빨')\n",
        "mean_word_predict_morpheme('씨빨')\n",
        "mean_word_predict_morpheme('ㅅ발')\n",
        "mean_word_predict_morpheme('ㅅㅣ발')\n",
        "mean_word_predict_morpheme('ㅆㅣ발')\n",
        "mean_word_predict_morpheme('ㅆ1발')\n",
        "mean_word_predict_morpheme('ㅅㅅ1발')\n",
        "mean_word_predict_morpheme('^^1발')\n",
        "mean_word_predict_morpheme('^^1발련')\n",
        "mean_word_predict_morpheme('^^1발놈')\n",
        "mean_word_predict_morpheme('人人ㅣ발')\n",
        "mean_word_predict_morpheme('씹알')\n",
        "mean_word_predict_morpheme('ㅅ시발')\n",
        "mean_word_predict_morpheme('씨ㅂ알')\n",
        "mean_word_predict_morpheme('시부랄')\n",
        "mean_word_predict_morpheme('시부럴')\n",
        "mean_word_predict_morpheme('시불탱')\n",
        "mean_word_predict_morpheme('슈발')\n",
        "mean_word_predict_morpheme('쉬발')\n",
        "mean_word_predict_morpheme('쉬빨')\n",
        "mean_word_predict_morpheme('색히')\n",
        "mean_word_predict_morpheme('색기')\n",
        "mean_word_predict_morpheme('세끼')\n",
        "mean_word_predict_morpheme('새퀴')\n",
        "mean_word_predict_morpheme('쉨')\n",
        "mean_word_predict_morpheme('샠')\n",
        "mean_word_predict_morpheme('ㅅㅐ끼')\n",
        "mean_word_predict_morpheme('ㅅㅐㄲㅣ')\n"
      ],
      "metadata": {
        "id": "m8Mo_tRCoo7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e62a64-76ee-4211-93c7-f7e6c791f7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99.91% 확률로 욕설입니다.\n",
            "\n",
            "60.37% 확률로 욕설입니다.\n",
            "\n",
            "55.36% 확률로 욕설이 아닙니다.\n",
            "\n",
            "60.37% 확률로 욕설입니다.\n",
            "\n",
            "77.54% 확률로 욕설입니다.\n",
            "\n",
            "84.58% 확률로 욕설입니다.\n",
            "\n",
            "57.59% 확률로 욕설이 아닙니다.\n",
            "\n",
            "67.89% 확률로 욕설이 아닙니다.\n",
            "\n",
            "90.76% 확률로 욕설입니다.\n",
            "\n",
            "85.31% 확률로 욕설입니다.\n",
            "\n",
            "62.13% 확률로 욕설이 아닙니다.\n",
            "\n",
            "92.52% 확률로 욕설입니다.\n",
            "\n",
            "89.50% 확률로 욕설이 아닙니다.\n",
            "\n",
            "76.02% 확률로 욕설이 아닙니다.\n",
            "\n",
            "98.30% 확률로 욕설입니다.\n",
            "\n",
            "99.94% 확률로 욕설입니다.\n",
            "\n",
            "56.80% 확률로 욕설입니다.\n",
            "\n",
            "87.19% 확률로 욕설이 아닙니다.\n",
            "\n",
            "60.66% 확률로 욕설이 아닙니다.\n",
            "\n",
            "66.15% 확률로 욕설이 아닙니다.\n",
            "\n",
            "85.68% 확률로 욕설입니다.\n",
            "\n",
            "64.67% 확률로 욕설이 아닙니다.\n",
            "\n",
            "86.95% 확률로 욕설이 아닙니다.\n",
            "\n",
            "89.84% 확률로 욕설이 아닙니다.\n",
            "\n",
            "52.94% 확률로 욕설이 아닙니다.\n",
            "\n",
            "87.31% 확률로 욕설이 아닙니다.\n",
            "\n",
            "86.95% 확률로 욕설이 아닙니다.\n",
            "\n",
            "86.70% 확률로 욕설이 아닙니다.\n",
            "\n",
            "50.55% 확률로 욕설이 아닙니다.\n",
            "\n",
            "82.85% 확률로 욕설이 아닙니다.\n",
            "\n",
            "86.95% 확률로 욕설이 아닙니다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TeICogYg11Nb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}